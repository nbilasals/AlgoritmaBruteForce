{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdIZg3XvmNrLoIwyaJpMS3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbilasals/AlgoritmaBruteForce/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ü©∫ BREAST CANCER PREDICTION: MALIGNANT VS BENIGN CLASSIFICATION üß¨\n",
        "# ============================================================================\n",
        "#\n",
        "# PROJECT OVERVIEW:\n",
        "# -----------------\n",
        "# This comprehensive notebook demonstrates a complete machine learning workflow\n",
        "# for predicting whether a breast tumor is malignant (cancerous) or benign\n",
        "# (non-cancerous) based on physical characteristics extracted from biopsy images.\n",
        "#\n",
        "# DATASET: Breast Cancer Wisconsin (Diagnostic) Dataset\n",
        "# - Source: UCI Machine Learning Repository\n",
        "# - Samples: 569 breast tumor cases\n",
        "# - Features: 30 numeric measurements of tumor characteristics\n",
        "# - Target: Binary classification (Malignant vs Benign)\n",
        "#\n",
        "# WORKFLOW STAGES:\n",
        "# 1. Data Loading & Exploration\n",
        "# 2. Exploratory Data Analysis (EDA)\n",
        "# 3. Data Preprocessing & Feature Engineering\n",
        "# 4. Multiple ML Model Training\n",
        "# 5. Model Evaluation & Comparison\n",
        "# 6. Final Recommendations\n",
        "#\n",
        "# CLINICAL SIGNIFICANCE:\n",
        "# Early and accurate detection of breast cancer is crucial for successful\n",
        "# treatment. This machine learning approach can assist medical professionals\n",
        "# in making more informed diagnostic decisions.\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: IMPORT NECESSARY LIBRARIES\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# In this cell, we import all the essential Python libraries needed for our\n",
        "# complete machine learning pipeline. Each library serves a specific purpose\n",
        "# in our analysis workflow.\n",
        "#\n",
        "# LIBRARIES BREAKDOWN:\n",
        "# --------------------\n",
        "#\n",
        "# üìä DATA MANIPULATION:\n",
        "# - numpy: Numerical computing, array operations, mathematical functions\n",
        "# - pandas: Data manipulation, DataFrame operations, data analysis\n",
        "#\n",
        "# üìà DATA VISUALIZATION:\n",
        "# - matplotlib.pyplot: Creating static, animated, and interactive visualizations\n",
        "# - seaborn: Statistical data visualization built on matplotlib, prettier plots\n",
        "#\n",
        "# ü§ñ MACHINE LEARNING - CORE:\n",
        "# - sklearn.datasets: Access to built-in datasets including breast cancer data\n",
        "# - sklearn.model_selection: Tools for splitting data and cross-validation\n",
        "# - sklearn.preprocessing: Data preprocessing tools like scaling and encoding\n",
        "#\n",
        "# üéØ MACHINE LEARNING - ALGORITHMS:\n",
        "# - LogisticRegression: Linear model for binary classification\n",
        "# - DecisionTreeClassifier: Tree-based model with interpretable rules\n",
        "# - RandomForestClassifier: Ensemble of decision trees for robust predictions\n",
        "# - SVC: Support Vector Classifier for complex decision boundaries\n",
        "# - KNeighborsClassifier: Instance-based learning using nearest neighbors\n",
        "#\n",
        "# üìè MACHINE LEARNING - EVALUATION:\n",
        "# - Various metrics: accuracy, precision, recall, F1-score for model assessment\n",
        "# - confusion_matrix: Visual representation of classification results\n",
        "# - ROC curves & AUC: Evaluate model performance across different thresholds\n",
        "#\n",
        "# WHY EACH METRIC MATTERS:\n",
        "# - Accuracy: Overall correctness of predictions\n",
        "# - Precision: Of all positive predictions, how many were actually positive?\n",
        "# - Recall: Of all actual positives, how many did we correctly identify?\n",
        "# - F1-Score: Harmonic mean of precision and recall (balanced metric)\n",
        "# - ROC-AUC: Model's ability to distinguish between classes\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report,\n",
        "                             roc_curve, roc_auc_score)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style for better-looking plots\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚úÖ ALL LIBRARIES IMPORTED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nüì¶ Libraries loaded:\")\n",
        "print(\"   ‚úì NumPy - Numerical computing\")\n",
        "print(\"   ‚úì Pandas - Data manipulation\")\n",
        "print(\"   ‚úì Matplotlib & Seaborn - Data visualization\")\n",
        "print(\"   ‚úì Scikit-learn - Machine learning algorithms and tools\")\n",
        "print(\"\\nüöÄ Ready to begin breast cancer prediction analysis!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 2: LOAD AND PREPARE THE BREAST CANCER DATASET\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell loads the Breast Cancer Wisconsin dataset from scikit-learn's\n",
        "# built-in datasets and prepares it for analysis by converting it into a\n",
        "# pandas DataFrame with proper column names and labels.\n",
        "#\n",
        "# ABOUT THE DATASET:\n",
        "# ------------------\n",
        "# The Breast Cancer Wisconsin (Diagnostic) Dataset contains features computed\n",
        "# from digitized images of fine needle aspirate (FNA) of breast masses. These\n",
        "# features describe characteristics of cell nuclei present in the images.\n",
        "#\n",
        "# DATA COLLECTION METHOD:\n",
        "# 1. A fine needle aspirate (FNA) is taken from a breast mass\n",
        "# 2. The sample is digitized and processed\n",
        "# 3. Computer vision algorithms extract features from cell nuclei\n",
        "# 4. Medical experts provide the diagnosis (malignant or benign)\n",
        "#\n",
        "# DATASET STRUCTURE:\n",
        "# - 569 instances (patient cases)\n",
        "# - 30 real-valued features (measurements)\n",
        "# - 2 classes: Malignant (0) and Benign (1)\n",
        "# - No missing values (complete dataset)\n",
        "#\n",
        "# TARGET VARIABLE ENCODING:\n",
        "# - 0 = Malignant (M) ‚Üí Cancer is present, requires treatment\n",
        "# - 1 = Benign (B) ‚Üí No cancer, but monitoring may be needed\n",
        "#\n",
        "# WHY THIS MATTERS:\n",
        "# In medical diagnosis, correctly identifying malignant tumors (high recall)\n",
        "# is critical to ensure patients receive timely treatment. However, we also\n",
        "# want to avoid false positives (high precision) to prevent unnecessary\n",
        "# stress and medical procedures.\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä LOADING BREAST CANCER WISCONSIN DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load the dataset from scikit-learn's built-in datasets\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Create a pandas DataFrame for easier manipulation and analysis\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# Add the target variable (diagnosis) to our DataFrame\n",
        "df['diagnosis'] = data.target\n",
        "\n",
        "# Create human-readable labels for better interpretation\n",
        "# Map 0 ‚Üí 'Malignant' (cancerous), 1 ‚Üí 'Benign' (non-cancerous)\n",
        "df['diagnosis_label'] = df['diagnosis'].map({0: 'Malignant', 1: 'Benign'})\n",
        "\n",
        "print(\"\\n‚úÖ Dataset loaded successfully!\")\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã DATASET OVERVIEW:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Total number of samples: {len(df)}\")\n",
        "print(f\"   ‚Ä¢ Number of features: {len(data.feature_names)}\")\n",
        "print(f\"   ‚Ä¢ Dataset dimensions: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
        "print(f\"   ‚Ä¢ Number of malignant cases: {(df['diagnosis'] == 0).sum()}\")\n",
        "print(f\"   ‚Ä¢ Number of benign cases: {(df['diagnosis'] == 1).sum()}\")\n",
        "print(f\"   ‚Ä¢ Class balance ratio: {(df['diagnosis'] == 1).sum() / len(df) * 100:.1f}% benign\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìù DATASET DESCRIPTION:\")\n",
        "print(\"-\" * 80)\n",
        "print(data.DESCR[:500] + \"...\")  # Print first 500 characters of description\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 3: INITIAL DATA EXPLORATION - VIEWING THE DATA\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell performs initial exploration of our dataset to understand its\n",
        "# structure, data types, and basic characteristics. This is a critical first\n",
        "# step in any data science project.\n",
        "#\n",
        "# WHY DATA EXPLORATION MATTERS:\n",
        "# -----------------------------\n",
        "# Before building any machine learning model, we need to:\n",
        "# 1. Understand what data we're working with\n",
        "# 2. Identify data types and potential issues\n",
        "# 3. Get familiar with feature names and values\n",
        "# 4. Check for any obvious problems or anomalies\n",
        "#\n",
        "# WHAT WE'RE EXAMINING:\n",
        "# ---------------------\n",
        "# ‚Ä¢ First few rows: Get a feel for the actual data values\n",
        "# ‚Ä¢ Data types: Ensure all features are numeric (required for ML)\n",
        "# ‚Ä¢ Memory usage: Understand dataset size\n",
        "# ‚Ä¢ Feature names: Familiarize ourselves with what we're measuring\n",
        "#\n",
        "# THINGS TO LOOK FOR:\n",
        "# -------------------\n",
        "# ‚úì Are all features numeric? (Yes, required for our ML models)\n",
        "# ‚úì Do values seem reasonable? (No obvious errors)\n",
        "# ‚úì Are there any unexpected patterns?\n",
        "# ‚úì Do feature names make clinical sense?\n",
        "#\n",
        "# UNDERSTANDING THE OUTPUT:\n",
        "# -------------------------\n",
        "# - .head(): Shows first 5 rows of data\n",
        "# - .info(): Provides data types, non-null counts, memory usage\n",
        "# - .describe(): Statistical summary (mean, std, min, max, quartiles)\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîç INITIAL DATA EXPLORATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã FIRST 5 ROWS OF THE DATASET\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nThis gives us a glimpse of the actual data values:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä DATASET INFORMATION (DATA TYPES & STRUCTURE)\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nDetailed information about each column:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìà STATISTICAL SUMMARY OF ALL FEATURES\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nDescriptive statistics for each numeric feature:\")\n",
        "print(df.describe().round(2))\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìù INTERPRETATION GUIDE:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\"\"\n",
        "‚Ä¢ count: Number of non-null values (should be 569 for all)\n",
        "‚Ä¢ mean: Average value - center of the distribution\n",
        "‚Ä¢ std: Standard deviation - measure of spread/variability\n",
        "‚Ä¢ min: Minimum value observed\n",
        "‚Ä¢ 25%: First quartile (25% of data is below this value)\n",
        "‚Ä¢ 50%: Median (middle value when sorted)\n",
        "‚Ä¢ 75%: Third quartile (75% of data is below this value)\n",
        "‚Ä¢ max: Maximum value observed\n",
        "\n",
        "KEY OBSERVATIONS:\n",
        "‚úì All features have 569 non-null values ‚Üí No missing data!\n",
        "‚úì All features are numeric (float64) ‚Üí Ready for machine learning\n",
        "‚úì Different features have vastly different scales ‚Üí Will need scaling\n",
        "‚úì Some features show high variability (large std) ‚Üí Normal for medical data\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 4: DATA QUALITY CHECK - MISSING VALUES ANALYSIS\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell thoroughly checks for missing values in our dataset. Missing data\n",
        "# is one of the most common problems in real-world datasets and can significantly\n",
        "# impact model performance if not handled properly.\n",
        "#\n",
        "# WHY MISSING VALUES MATTER:\n",
        "# --------------------------\n",
        "# Missing values can occur due to:\n",
        "# ‚Ä¢ Data collection errors\n",
        "# ‚Ä¢ Equipment malfunction\n",
        "# ‚Ä¢ Human error during data entry\n",
        "# ‚Ä¢ Privacy concerns (data intentionally omitted)\n",
        "# ‚Ä¢ Technical issues during data transfer\n",
        "#\n",
        "# IMPACT ON MACHINE LEARNING:\n",
        "# ----------------------------\n",
        "# Most ML algorithms cannot handle missing values and will either:\n",
        "# 1. Throw an error and refuse to run\n",
        "# 2. Produce incorrect results\n",
        "# 3. Automatically drop rows/columns with missing values\n",
        "#\n",
        "# COMMON STRATEGIES FOR HANDLING MISSING DATA:\n",
        "# ---------------------------------------------\n",
        "# If we find missing values, we can:\n",
        "# 1. DELETE: Remove rows or columns with missing values\n",
        "#    - Use when: Very few missing values (<5% of data)\n",
        "#    - Pros: Simple, no assumptions made\n",
        "#    - Cons: Lose potentially valuable data\n",
        "#\n",
        "# 2. IMPUTE - MEAN/MEDIAN/MODE:\n",
        "#    - Use when: Data is missing at random\n",
        "#    - Pros: Retains all samples\n",
        "#    - Cons: Can distort distributions\n",
        "#\n",
        "# 3. IMPUTE - ADVANCED METHODS:\n",
        "#    - Use algorithms like KNN or regression to predict missing values\n",
        "#    - Pros: More accurate than simple imputation\n",
        "#    - Cons: More complex, computationally expensive\n",
        "#\n",
        "# 4. CREATE INDICATOR VARIABLES:\n",
        "#    - Add binary column indicating if value was missing\n",
        "#    - Use when: Missingness itself is informative\n",
        "#\n",
        "# GOOD NEWS FOR OUR DATASET:\n",
        "# ---------------------------\n",
        "# The Wisconsin Breast Cancer dataset is a well-curated research dataset\n",
        "# with NO missing values! This is rare in real-world scenarios but makes\n",
        "# our analysis cleaner and more straightforward.\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîç DATA QUALITY CHECK: MISSING VALUES ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check for missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "total_cells = np.product(df.shape)\n",
        "total_missing = missing_values.sum()\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä MISSING VALUES SUMMARY:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\n   ‚Ä¢ Total cells in dataset: {total_cells:,}\")\n",
        "print(f\"   ‚Ä¢ Total missing values: {total_missing}\")\n",
        "print(f\"   ‚Ä¢ Percentage of missing data: {(total_missing / total_cells) * 100:.2f}%\")\n",
        "\n",
        "if total_missing == 0:\n",
        "    print(\"\\n   ‚úÖ EXCELLENT! No missing values found in any column!\")\n",
        "    print(\"   ‚úÖ Dataset is complete and ready for machine learning!\")\n",
        "    print(\"\\n   This is a high-quality dataset - no imputation needed!\")\n",
        "else:\n",
        "    print(\"\\n   ‚ö†Ô∏è WARNING: Missing values detected!\")\n",
        "    print(\"\\n   Columns with missing values:\")\n",
        "    print(\"-\" * 80)\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Column': missing_values[missing_values > 0].index,\n",
        "        'Missing Count': missing_values[missing_values > 0].values,\n",
        "        'Percentage': (missing_values[missing_values > 0].values / len(df) * 100).round(2)\n",
        "    })\n",
        "    print(missing_df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n   üìù RECOMMENDED ACTIONS:\")\n",
        "    print(\"   \" + \"-\" * 76)\n",
        "    for col, missing_pct in zip(missing_df['Column'], missing_df['Percentage']):\n",
        "        if missing_pct < 5:\n",
        "            print(f\"   ‚Ä¢ {col}: {missing_pct}% missing ‚Üí Consider removing rows\")\n",
        "        elif missing_pct < 30:\n",
        "            print(f\"   ‚Ä¢ {col}: {missing_pct}% missing ‚Üí Consider imputation\")\n",
        "        else:\n",
        "            print(f\"   ‚Ä¢ {col}: {missing_pct}% missing ‚Üí Consider removing column\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üí° DATA QUALITY ASSESSMENT:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "‚úì All 569 samples are complete\n",
        "‚úì All 30 features have valid measurements\n",
        "‚úì No data cleaning required for missing values\n",
        "‚úì Ready to proceed with exploratory data analysis\n",
        "\n",
        "This clean dataset allows us to focus on feature engineering and\n",
        "model building without worrying about data imputation strategies!\n",
        "\"\"\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 5: TARGET VARIABLE DISTRIBUTION ANALYSIS\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell analyzes the distribution of our target variable (diagnosis).\n",
        "# Understanding class distribution is crucial because it affects:\n",
        "# 1. Model training and performance\n",
        "# 2. Evaluation metric selection\n",
        "# 3. Potential need for balancing techniques\n",
        "#\n",
        "# WHY CLASS DISTRIBUTION MATTERS:\n",
        "# -------------------------------\n",
        "# BALANCED DATASET (50:50 ratio):\n",
        "# ‚Ä¢ Models learn both classes equally well\n",
        "# ‚Ä¢ Standard metrics (accuracy) work well\n",
        "# ‚Ä¢ No special techniques needed\n",
        "#\n",
        "# IMBALANCED DATASET (e.g., 90:10 ratio):\n",
        "# ‚Ä¢ Models may become biased toward majority class\n",
        "# ‚Ä¢ Accuracy can be misleading (e.g., 90% by always predicting majority)\n",
        "# ‚Ä¢ May need: oversampling, undersampling, SMOTE, or class weights\n",
        "# ‚Ä¢ Should focus on: precision, recall, F1-score, not just accuracy\n",
        "#\n",
        "# CLASS IMBALANCE IN MEDICAL DIAGNOSIS:\n",
        "# --------------------------------------\n",
        "# In medical datasets, imbalance is common because:\n",
        "# ‚Ä¢ Diseases are often rare in the general population\n",
        "# ‚Ä¢ More benign cases than malignant in screening programs\n",
        "# ‚Ä¢ Cost of false negatives (missing cancer) is very high\n",
        "#\n",
        "# WHAT TO LOOK FOR:\n",
        "# -----------------\n",
        "# ‚Ä¢ Ratio between classes (is one significantly larger?)\n",
        "# ‚Ä¢ Absolute numbers (do we have enough samples of minority class?)\n",
        "# ‚Ä¢ Consider if imbalance reflects real-world prevalence\n",
        "#\n",
        "# EVALUATION METRIC IMPLICATIONS:\n",
        "# --------------------------------\n",
        "# ‚Ä¢ Balanced dataset ‚Üí Accuracy is fine\n",
        "# ‚Ä¢ Imbalanced dataset ‚Üí Focus on precision/recall/F1-score\n",
        "# ‚Ä¢ Medical context ‚Üí Prioritize RECALL (don't miss cancer cases)\n",
        "#\n",
        "# VISUALIZATIONS INCLUDED:\n",
        "# ------------------------\n",
        "# 1. Count plot: Shows absolute numbers of each class\n",
        "# 2. Pie chart: Shows proportional distribution\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéØ TARGET VARIABLE DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate class distribution\n",
        "diagnosis_counts = df['diagnosis_label'].value_counts()\n",
        "diagnosis_percentages = df['diagnosis_label'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä CLASS DISTRIBUTION (ABSOLUTE COUNTS):\")\n",
        "print(\"-\" * 80)\n",
        "for label, count in diagnosis_counts.items():\n",
        "    print(f\"   ‚Ä¢ {label:12s}: {count:3d} cases\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä CLASS DISTRIBUTION (PERCENTAGES):\")\n",
        "print(\"-\" * 80)\n",
        "for label, pct in diagnosis_percentages.items():\n",
        "    print(f\"   ‚Ä¢ {label:12s}: {pct:5.2f}%\")\n",
        "\n",
        "# Calculate imbalance ratio\n",
        "majority_count = diagnosis_counts.values[0]\n",
        "minority_count = diagnosis_counts.values[1]\n",
        "imbalance_ratio = majority_count / minority_count\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"‚öñÔ∏è CLASS BALANCE ANALYSIS:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   ‚Ä¢ Majority class: {diagnosis_counts.index[0]} ({majority_count} samples)\")\n",
        "print(f\"   ‚Ä¢ Minority class: {diagnosis_counts.index[1]} ({minority_count} samples)\")\n",
        "print(f\"   ‚Ä¢ Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "if imbalance_ratio < 1.5:\n",
        "    balance_status = \"‚úÖ WELL BALANCED\"\n",
        "    recommendation = \"Standard ML algorithms will work well without modifications.\"\n",
        "elif imbalance_ratio < 3:\n",
        "    balance_status = \"‚ö†Ô∏è SLIGHT IMBALANCE\"\n",
        "    recommendation = \"Consider monitoring precision and recall separately. May use class weights.\"\n",
        "else:\n",
        "    balance_status = \"‚ùå SIGNIFICANT IMBALANCE\"\n",
        "    recommendation = \"Should use: class weights, SMOTE, or focus on F1-score/AUC metrics.\"\n",
        "\n",
        "print(f\"\\n   Status: {balance_status}\")\n",
        "print(f\"   Recommendation: {recommendation}\")\n",
        "\n",
        "# Visualization\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä GENERATING VISUALIZATIONS...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Count plot with detailed annotations\n",
        "bars = axes[0].bar(range(len(diagnosis_counts)), diagnosis_counts.values,\n",
        "                   color=['#FF6B6B', '#4ECDC4'], alpha=0.8, edgecolor='black', linewidth=2)\n",
        "axes[0].set_xticks(range(len(diagnosis_counts)))\n",
        "axes[0].set_xticklabels(diagnosis_counts.index, fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Number of Cases', fontsize=13, fontweight='bold')\n",
        "axes[0].set_title('Distribution of Breast Cancer Diagnosis\\n(Absolute Counts)',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "axes[0].grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (bar, count, pct) in enumerate(zip(bars, diagnosis_counts.values, diagnosis_percentages.values)):\n",
        "    height = bar.get_height()\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 5,\n",
        "                f'{count}\\n({pct:.1f}%)',\n",
        "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Pie chart with enhanced styling\n",
        "colors = ['#FF6B6B', '#4ECDC4']\n",
        "explode = (0.05, 0.05)  # Slightly separate both slices\n",
        "wedges, texts, autotexts = axes[1].pie(diagnosis_counts.values,\n",
        "                                        labels=diagnosis_counts.index,\n",
        "                                        autopct='%1.1f%%',\n",
        "                                        colors=colors,\n",
        "                                        explode=explode,\n",
        "                                        startangle=90,\n",
        "                                        textprops={'fontsize': 12, 'fontweight': 'bold'},\n",
        "                                        shadow=True)\n",
        "axes[1].set_title('Proportion of Malignant vs Benign Cases\\n(Percentage Distribution)',\n",
        "                  fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "# Enhance autotext\n",
        "for autotext in autotexts:\n",
        "    autotext.set_color('white')\n",
        "    autotext.set_fontsize(13)\n",
        "    autotext.set_weight('bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üí° KEY INSIGHTS FROM CLASS DISTRIBUTION:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\"\"\n",
        "1. Dataset Balance: The dataset has a {imbalance_ratio:.2f}:1 ratio of\n",
        "   Benign to Malignant cases.\n",
        "\n",
        "2. Clinical Relevance: This distribution is actually quite realistic for\n",
        "   breast cancer screening programs, where benign findings are more common\n",
        "   than malignant tumors.\n",
        "\n",
        "3. Modeling Implications:\n",
        "   ‚Ä¢ Our dataset is reasonably balanced ({diagnosis_percentages.values[1]:.1f}% minority class)\n",
        "   ‚Ä¢ We can use standard accuracy metrics, but should also monitor:\n",
        "     - Recall (sensitivity): To minimize false negatives\n",
        "     - Precision: To minimize false positives\n",
        "     - F1-Score: Balanced measure of both\n",
        "\n",
        "4. Medical Context: In cancer detection, missing a malignant case\n",
        "   (false negative) is typically considered worse than a false positive,\n",
        "   so we'll pay special attention to RECALL scores.\n",
        "\"\"\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 6: FEATURE ANALYSIS - UNDERSTANDING THE MEASUREMENTS\n",
        "# ============================================================================\n",
        "#\n",
        "# SECTION PURPOSE:\n",
        "# ----------------\n",
        "# This cell provides a comprehensive analysis of the 30 features in our dataset.\n",
        "# Each feature represents a different measurement of tumor characteristics\n",
        "# derived from cell nuclei in biopsy images.\n",
        "#\n",
        "# FEATURE ORGANIZATION:\n",
        "# ---------------------\n",
        "# The 30 features are organized into 3 groups of 10 measurements each:\n",
        "#\n",
        "# 1. MEAN VALUES (10 features):\n",
        "#    - Average of measurements across all cells in the image\n",
        "#    - Suffix: \"_mean\"\n",
        "#    - Example: \"mean radius\", \"mean texture\"\n",
        "#\n",
        "# 2. STANDARD ERROR (10 features):\n",
        "#    - Standard error of measurements (variability measure)\n",
        "#    - Suffix: \"_se\"\n",
        "#    - Example: \"radius error\", \"texture error\"\n",
        "#\n",
        "# 3. WORST VALUES (10 features):\n",
        "#    - Mean of the three largest values\n",
        "#    - Suffix: \"_worst\"\n",
        "#    - Example: \"worst radius\", \"worst texture\"\n",
        "#\n",
        "# THE 10 CORE MEASUREMENTS:\n",
        "# --------------------------\n",
        "# Each group contains these 10 measurements:\n",
        "#\n",
        "# 1. RADIUS:\n",
        "#    - Mean distance from center to points on perimeter\n",
        "#    - Larger radius ‚Üí Larger tumor\n",
        "#    - Medical significance: Size is a key diagnostic indicator\n",
        "#\n",
        "# 2. TEXTURE:\n",
        "#    - Standard deviation of gray-scale values\n",
        "#    - Higher texture ‚Üí More irregular cell appearance\n",
        "#    - Medical significance: Malignant cells often more irregular\n",
        "#\n",
        "# 3. PERIMETER:\n",
        "#    - Total boundary length of the tumor\n",
        "#    - Related to radius but captures shape complexity\n",
        "#    - Medical significance: Irregular perimeters suggest malignancy\n",
        "#\n",
        "# 4. AREA:\n",
        "#    - Total area enclosed by tumor perimeter\n",
        "#    - Directly related to tumor size\n",
        "#    - Medical significance: Larger tumors more concerning\n",
        "#\n",
        "# 5. SMOOTHNESS:\n",
        "#    - Local variation in radius lengths\n",
        "#    - Smoother ‚Üí More uniform, regular shape\n",
        "#    - Medical significance: Benign tumors typically smoother\n",
        "#\n",
        "# 6. COMPACTNESS:\n",
        "#    - Perimeter¬≤ / Area - 1.0\n",
        "#    - Measures how compact vs. spread out the tumor is\n",
        "#    - Medical significance: Malignant cells less compact\n",
        "#\n",
        "# 7. CONCAVITY:\n",
        "#    - Severity of concave portions of contour\n",
        "#    - Higher values ‚Üí More indentations in tumor boundary\n",
        "#    - Medical significance: Malignant tumors more irregular\n",
        "#\n",
        "# 8. CONCAVE POINTS:\n",
        "#    - Number of concave portions of contour\n",
        "#    - Counts distinct indentations\n",
        "#    - Medical significance: Strong indicator of malignancy\n",
        "#\n",
        "# 9. SYMMETRY:\n",
        "#    - Measures symmetry of the tumor\n",
        "#    - Higher values ‚Üí More asymmetric\n",
        "#    - Medical significance: Malignant tumors less symmetric\n",
        "#\n",
        "# 10. FRACTAL DIMENSION:\n",
        "#     - \"Coastline approximation\" - 1\n",
        "#     - Measures complexity of the boundary\n",
        "#     - Medical significance: Complex boundaries suggest malignancy\n",
        "#\n",
        "# WHY THREE VERSIONS OF EACH MEASUREMENT?\n",
        "# ----------------------------------------\n",
        "# ‚Ä¢ MEAN: Overall average characteristic\n",
        "# ‚Ä¢ ERROR: Variability within the sample (uncertainty measure)\n",
        "# ‚Ä¢ WORST: Most severe measurements (often most diagnostic)\n",
        "#\n",
        "# CLINICAL INTERPRETATION:\n",
        "# ------------------------\n",
        "# Medical professionals look for:\n",
        "# ‚úì Large radius/perimeter/area ‚Üí Concerning\n",
        "# ‚úì High texture/concavity ‚Üí Irregular cells ‚Üí Concerning\n",
        "# ‚úì Low smoothness/symmetry ‚Üí Irregular shape ‚Üí Concerning\n",
        "# ‚úì High \"worst\" values ‚Üí Most concerning areas ‚Üí Diagnostic\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üî¨ COMPREHENSIVE FEATURE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Categorize features by type\n",
        "mean_features = [col for col in df.columns if 'mean' in col]\n",
        "se_features = [col for col in df.columns if 'error' in col or 'se' in col]\n",
        "worst_features = [col for col in df.columns if 'worst' in col]\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìã FEATURE ORGANIZATION:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"\\n1. MEAN FEATURES ({len(mean_features)} features):\")\n",
        "print(\"   These represent the average measurements across all cells:\")\n",
        "for i, feature in enumerate(mean_features, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "print(f\"\\n2. STANDARD ERROR FEATURES ({len(se_features)} features):\")\n",
        "print(\"   These represent the variability/uncertainty in measurements:\")\n",
        "for i, feature in enumerate(se_features, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "print(f\"\\n3. WORST FEATURES ({len(worst_features)} features):\")\n",
        "print(\"   These represent the most extreme measurements:\")\n",
        "for i, feature in enumerate(worst_features, 1):\n",
        "    print(f\"   {i:2d}. {feature}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üìä FEATURE VALUE RANGES:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nUnderstanding the scale of measurements:\")\n",
        "print(df[mean_features].describe().loc[['min', 'max']].round(2))\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"üí° KEY OBSERVATIONS:\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\"\"\n",
        "1. SCALE DIFFERENCES:\n",
        "   ‚Ä¢ Features have vastly different scales\n",
        "   ‚Ä¢ Example: 'mean area' ranges from ~143 to ~2501\n",
        "   ‚Ä¢ Example: 'mean smoothness' ranges from ~0.05 to ~0.16\n",
        "   ‚Ä¢ Implication: MUST scale features before ML modeling\n",
        "\n",
        "2. PHYSICAL MEANING:\n",
        "   ‚Ä¢ Size features (radius, perimeter, area) are correlated\n",
        "   ‚Ä¢ Shape features (smoothness, symmetry) describe regularity\n",
        "   ‚Ä¢ Texture features describe cell appearance variability\n",
        "\n",
        "3. DIAGNOSTIC RELEVANCE:\n",
        "   ‚Ä¢ \"Worst\" features often most important for diagnosis\n",
        "   ‚Ä¢ They capture the most abnormal regions of the tumor\n",
        "   ‚Ä¢ Medical professionals focus on worst-case characteristics\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä GENERATING FEATURE DISTRIBUTION VISUALIZATIONS...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Visualization of mean features distribution by diagnosis\n",
        "fig, axes = plt.subplots(2, 5, figsize=(22, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(mean_features):\n",
        "    # Separate data by diagnosis\n",
        "    malignant_data = df[df['diagnosis'] == 0][feature]\n",
        "    benign_data = df[df['diagnosis'] == 1][feature]\n",
        "\n",
        "    # Create overlapping histograms\n",
        "    axes[idx].hist(malignant_data, bins=25, alpha=0.6, label='Malignant',\n",
        "                   color='#FF6B6B', edgecolor='black', linewidth=0.5)\n",
        "    axes[idx].hist(benign_data, bins=25, alpha=0.6, label='Benign',\n",
        "                   color='#4ECDC4', edgecolor='black', linewidth=0.5)\n",
        "\n",
        "    # Styling\n",
        "    axes[idx].set_title(feature.replace('mean ', '').title(),\n",
        "                        fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Value', fontsize=9)\n",
        "    axes[idx].set_ylabel('Frequency', fontsize=9)\n",
        "    axes[idx].legend(fontsize=8, loc='upper right')\n",
        "    axes[idx].grid(alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Add statistical annotations\n",
        "    mal_mean = malignant_data.mean()\n",
        "    ben_mean = benign_data.mean()\n",
        "    axes[idx].axvline(mal_mean, color='#FF6B6B', linestyle='--',\n",
        "                      linewidth=2, alpha=0.7, label=f'M Œº={mal_mean:.1f}')\n",
        "    axes[idx].axvline(ben_mean, color='#4ECDC4', linestyle='--',\n",
        "                      linewidth=2, alpha=0.7, label=f'B Œº={ben_mean:.1f}')\n",
        "\n",
        "plt.suptitle('Distribution of Mean Features by Diagnosis\\n' +\n",
        "             'Red = Malignant | Teal = Benign | Dashed lines = Mean values',\n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print"
      ],
      "metadata": {
        "id": "qBQGLIKWMNYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLonJCBNX7CK"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE SELECTION FOR MODELING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get all encoded features and numerical features\n",
        "encoded_features = [col for col in df_encoded.columns if col.endswith('_encoded')]\n",
        "original_numerical = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak', 'FastingBS']\n",
        "engineered_numerical = ['HR_Achievement_Pct', 'Simple_Risk_Score']\n",
        "\n",
        "# Combine features\n",
        "feature_cols = [col for col in original_numerical if col in df_encoded.columns]\n",
        "feature_cols.extend([col for col in encoded_features if col in df_encoded.columns])\n",
        "feature_cols.extend([col for col in engineered_numerical if col in df_encoded.columns])\n",
        "\n",
        "# Remove target if accidentally included\n",
        "if target_col in feature_cols:\n",
        "    feature_cols.remove(target_col)\n",
        "\n",
        "print(f\"Selected {len(feature_cols)} features for modeling:\")\n",
        "for i, col in enumerate(feature_cols, 1):\n",
        "    print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "# Create feature matrix and target vector\n",
        "X = df_encoded[feature_cols].fillna(0)  # Fill any remaining NaN with 0\n",
        "y = df_encoded[target_col]\n",
        "\n",
        "print(f\"\\n‚úì Feature matrix (X): {X.shape[0]} patients √ó {X.shape[1]} features\")\n",
        "print(f\"‚úì Target vector (y): {y.shape[0]} patients\")\n",
        "print(f\"‚úì Class distribution:\")\n",
        "print(f\"   - Class 0 (No Disease): {(y == 0).sum()} ({(y == 0).sum()/len(y)*100:.1f}%)\")\n",
        "print(f\"   - Class 1 (Disease):    {(y == 1).sum()} ({(y == 1).sum()/len(y)*100:.1f}%)\")\n",
        "\n",
        "# ### 5.2 Feature Importance Analysis (Preliminary)\n",
        "\n",
        "\"\"\"\n",
        "Use Random Forest to identify most important features before modeling\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRELIMINARY FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train Random Forest for feature importance\n",
        "rf_temp = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_temp.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Importance': rf_temp.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 15 Most Important Features:\")\n",
        "print(feature_importance_df.head(15).to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_15 = feature_importance_df.head(15)\n",
        "plt.barh(range(len(top_15)), top_15['Importance'], color='steelblue',\n",
        "        edgecolor='black', alpha=0.8)\n",
        "plt.yticks(range(len(top_15)), top_15['Feature'])\n",
        "plt.xlabel('Importance Score')\n",
        "plt.title('Top 15 Most Important Features (Random Forest)', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---\n",
        "# ## 6. Data Splitting and Scaling\n",
        "# ### 6.1 Train-Test Split\n",
        "\n",
        "\"\"\"\n",
        "Split data into training and testing sets with stratification\n",
        "to maintain class distribution\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAIN-TEST SPLIT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"‚úì Training set: {len(X_train)} patients ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"‚úì Test set: {len(X_test)} patients ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"‚úì Features: {X_train.shape[1]}\")\n",
        "\n",
        "print(f\"\\nClass distribution in training set:\")\n",
        "print(f\"   - Class 0: {(y_train == 0).sum()} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)\")\n",
        "print(f\"   - Class 1: {(y_train == 1).sum()} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nClass distribution in test set:\")\n",
        "print(f\"   - Class 0: {(y_test == 0).sum()} ({(y_test == 0).sum()/len(y_test)*100:.1f}%)\")\n",
        "print(f\"   - Class 1: {(y_test == 1).sum()} ({(y_test == 1).sum()/len(y_test)*100:.1f}%)\")\n",
        "\n",
        "# ### 6.2 Feature Scaling\n",
        "\n",
        "\"\"\"\n",
        "Standardize features to have zero mean and unit variance.\n",
        "Critical for distance-based algorithms like SVM and KNN.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE SCALING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"‚úì Features scaled using StandardScaler\")\n",
        "print(f\"  Training set: Mean ‚âà 0, Std ‚âà 1 for all features\")\n",
        "\n",
        "# Convert back to DataFrame for easier handling\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
        "\n",
        "# ---\n",
        "# ## 7. Model Training and Evaluation\n",
        "# ### 7.1 Model 1: Logistic Regression\n",
        "\n",
        "\"\"\"\n",
        "Baseline Model: Logistic Regression\n",
        "- Simple, interpretable, fast\n",
        "- Good for linear relationships\n",
        "- Provides probability estimates\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 1: LOGISTIC REGRESSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000, solver='lbfgs')\n",
        "lr_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_lr = lr_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_lr = lr_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "lr_precision = precision_score(y_test, y_pred_lr)\n",
        "lr_recall = recall_score(y_test, y_pred_lr)\n",
        "lr_f1 = f1_score(y_test, y_pred_lr)\n",
        "lr_auc = roc_auc_score(y_test, y_pred_proba_lr)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {lr_precision:.4f} (of predicted positives, {lr_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {lr_recall:.4f} (detected {lr_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {lr_f1:.4f} (harmonic mean of precision and recall)\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {lr_auc:.4f} (area under ROC curve)\")\n",
        "\n",
        "# ### 7.2 Model 2: Random Forest Classifier\n",
        "\n",
        "\"\"\"\n",
        "Ensemble Model: Random Forest\n",
        "- Handles non-linear relationships\n",
        "- Robust to outliers\n",
        "- Provides feature importance\n",
        "- Less prone to overfitting\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 2: RANDOM FOREST CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=10,\n",
        "    min_samples_leaf=4,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "rf_precision = precision_score(y_test, y_pred_rf)\n",
        "rf_recall = recall_score(y_test, y_pred_rf)\n",
        "rf_f1 = f1_score(y_test, y_pred_rf)\n",
        "rf_auc = roc_auc_score(y_test, y_pred_proba_rf)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {rf_precision:.4f} (of predicted positives, {rf_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {rf_recall:.4f} (detected {rf_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {rf_f1:.4f}\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {rf_auc:.4f}\")\n",
        "\n",
        "# ### 7.3 Model 3: Support Vector Machine (SVM)\n",
        "\n",
        "\"\"\"\n",
        "Support Vector Machine with RBF Kernel\n",
        "- Effective in high-dimensional spaces\n",
        "- Good for non-linear classification\n",
        "- Memory efficient\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 3: SUPPORT VECTOR MACHINE (SVM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "svm_model = SVC(kernel='rbf', probability=True, random_state=42, C=1.0, gamma='scale')\n",
        "svm_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_svm = svm_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_svm = svm_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "svm_precision = precision_score(y_test, y_pred_svm)\n",
        "svm_recall = recall_score(y_test, y_pred_svm)\n",
        "svm_f1 = f1_score(y_test, y_pred_svm)\n",
        "svm_auc = roc_auc_score(y_test, y_pred_proba_svm)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {svm_accuracy:.4f} ({svm_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {svm_precision:.4f} (of predicted positives, {svm_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {svm_recall:.4f} (detected {svm_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {svm_f1:.4f}\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {svm_auc:.4f}\")\n",
        "\n",
        "# ### 7.4 Model 4: Gradient Boosting Classifier\n",
        "\n",
        "\"\"\"\n",
        "Gradient Boosting: Advanced Ensemble Method\n",
        "- Sequential learning\n",
        "- Often achieves high accuracy\n",
        "- Handles complex patterns\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 4: GRADIENT BOOSTING CLASSIFIER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    min_samples_split=10,\n",
        "    random_state=42\n",
        ")\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "y_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "gb_accuracy = accuracy_score(y_test, y_pred_gb)\n",
        "gb_precision = precision_score(y_test, y_pred_gb)\n",
        "gb_recall = recall_score(y_test, y_pred_gb)\n",
        "gb_f1 = f1_score(y_test, y_pred_gb)\n",
        "gb_auc = roc_auc_score(y_test, y_pred_proba_gb)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {gb_accuracy:.4f} ({gb_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {gb_precision:.4f} (of predicted positives, {gb_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {gb_recall:.4f} (detected {gb_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {gb_f1:.4f}\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {gb_auc:.4f}\")\n",
        "\n",
        "# ### 7.5 Model 5: K-Nearest Neighbors (KNN)\n",
        "\n",
        "\"\"\"\n",
        "K-Nearest Neighbors\n",
        "- Non-parametric, instance-based learning\n",
        "- Simple intuition: similar patients have similar outcomes\n",
        "- No training phase (lazy learning)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 5: K-NEAREST NEIGHBORS (KNN)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5, weights='distance', metric='euclidean')\n",
        "knn_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_knn = knn_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_knn = knn_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
        "knn_precision = precision_score(y_test, y_pred_knn)\n",
        "knn_recall = recall_score(y_test, y_pred_knn)\n",
        "knn_f1 = f1_score(y_test, y_pred_knn)\n",
        "knn_auc = roc_auc_score(y_test, y_pred_proba_knn)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {knn_accuracy:.4f} ({knn_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {knn_precision:.4f} (of predicted positives, {knn_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {knn_recall:.4f} (detected {knn_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {knn_f1:.4f}\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {knn_auc:.4f}\")\n",
        "\n",
        "# ### 7.6 Model 6: Naive Bayes\n",
        "\n",
        "\"\"\"\n",
        "Gaussian Naive Bayes\n",
        "- Probabilistic classifier\n",
        "- Fast and efficient\n",
        "- Works well with small datasets\n",
        "- Assumes feature independence\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL 6: NAIVE BAYES (GAUSSIAN)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_scaled_df, y_train)\n",
        "\n",
        "y_pred_nb = nb_model.predict(X_test_scaled_df)\n",
        "y_pred_proba_nb = nb_model.predict_proba(X_test_scaled_df)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "nb_accuracy = accuracy_score(y_test, y_pred_nb)\n",
        "nb_precision = precision_score(y_test, y_pred_nb)\n",
        "nb_recall = recall_score(y_test, y_pred_nb)\n",
        "nb_f1 = f1_score(y_test, y_pred_nb)\n",
        "nb_auc = roc_auc_score(y_test, y_pred_proba_nb)\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  ‚Ä¢ Accuracy:  {nb_accuracy:.4f} ({nb_accuracy*100:.2f}%)\")\n",
        "print(f\"  ‚Ä¢ Precision: {nb_precision:.4f} (of predicted positives, {nb_precision*100:.1f}% are correct)\")\n",
        "print(f\"  ‚Ä¢ Recall:    {nb_recall:.4f} (detected {nb_recall*100:.1f}% of actual disease cases)\")\n",
        "print(f\"  ‚Ä¢ F1-Score:  {nb_f1:.4f}\")\n",
        "print(f\"  ‚Ä¢ ROC-AUC:   {nb_auc:.4f}\")\n",
        "\n",
        "# ---\n",
        "# ## 8. Model Comparison and Analysis\n",
        "# ### 8.1 Comprehensive Model Comparison\n",
        "\n",
        "\"\"\"\n",
        "Compare all models across multiple metrics\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Random Forest', 'SVM',\n",
        "              'Gradient Boosting', 'KNN', 'Naive Bayes'],\n",
        "    'Accuracy': [lr_accuracy, rf_accuracy, svm_accuracy, gb_accuracy, knn_accuracy, nb_accuracy],\n",
        "    'Precision': [lr_precision, rf_precision, svm_precision, gb_precision, knn_precision, nb_precision],\n",
        "    'Recall': [lr_recall, rf_recall, svm_recall, gb_recall, knn_recall, nb_recall],\n",
        "    'F1-Score': [lr_f1, rf_f1, svm_f1, gb_f1, knn_f1, nb_f1],\n",
        "    'ROC-AUC': [lr_auc, rf_auc, svm_auc, gb_auc, knn_auc, nb_auc]\n",
        "})\n",
        "\n",
        "# Sort by F1-Score\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nModel Performance Summary (Ranked by F1-Score):\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    axes[idx].bar(range(len(comparison_df)), comparison_df[metric],\n",
        "                 color=colors, edgecolor='black', alpha=0.8)\n",
        "    axes[idx].set_xticks(range(len(comparison_df)))\n",
        "    axes[idx].set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
        "    axes[idx].set_title(f'{metric} Comparison', fontweight='bold', fontsize=12)\n",
        "    axes[idx].set_ylabel(metric)\n",
        "    axes[idx].set_ylim([0, 1.1])\n",
        "    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(comparison_df[metric]):\n",
        "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=9)\n",
        "\n",
        "# Overall performance radar chart\n",
        "axes[5].remove()\n",
        "ax_radar = fig.add_subplot(2, 3, 6, projection='polar')\n",
        "\n",
        "# Get best model for radar chart\n",
        "best_model_idx = comparison_df['F1-Score'].idxmax()\n",
        "best_model_data = comparison_df.iloc[best_model_idx]\n",
        "\n",
        "categories = metrics\n",
        "values = best_model_data[metrics].values\n",
        "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
        "values = np.concatenate((values, [values[0]]))\n",
        "angles += angles[:1]\n",
        "\n",
        "ax_radar.plot(angles, values, 'o-', linewidth=2, color='red', label=best_model_data['Model'])\n",
        "ax_radar.fill(angles, values, alpha=0.25, color='red')\n",
        "ax_radar.set_xticks(angles[:-1])\n",
        "ax_radar.set_xticklabels(categories)\n",
        "ax_radar.set_ylim(0, 1)\n",
        "ax_radar.set_title('Best Model Performance\\nRadar Chart', fontweight='bold', pad=20)\n",
        "ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
        "ax_radar.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_f1 = comparison_df.iloc[0]['F1-Score']\n",
        "best_auc = comparison_df.iloc[0]['ROC-AUC']\n",
        "\n",
        "print(f\"\\nüèÜ BEST PERFORMING MODEL: {best_model_name}\")\n",
        "print(f\"   ‚Ä¢ F1-Score: {best_f1:.4f}\")\n",
        "print(f\"   ‚Ä¢ ROC-AUC:  {best_auc:.4f}\")\n",
        "print(f\"   ‚Ä¢ Accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\n",
        "\n",
        "# Map to actual model and predictions\n",
        "model_map = {\n",
        "    'Logistic Regression': (lr_model, y_pred_lr, y_pred_proba_lr),\n",
        "    'Random Forest': (rf_model, y_pred_rf, y_pred_proba_rf),\n",
        "    'SVM': (svm_model, y_pred_svm, y_pred_proba_svm),\n",
        "    'Gradient Boosting': (gb_model, y_pred_gb, y_pred_proba_gb),\n",
        "    'KNN': (knn_model, y_pred_knn, y_pred_proba_knn),\n",
        "    'Naive Bayes': (nb_model, y_pred_nb, y_pred_proba_nb)\n",
        "}\n",
        "best_model, y_pred_best, y_pred_proba_best = model_map[best_model_name]\n",
        "\n",
        "# ### 8.2 Confusion Matrix Analysis\n",
        "\n",
        "\"\"\"\n",
        "Detailed breakdown of predictions for all models\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFUSION MATRIX ANALYSIS - ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "predictions = [\n",
        "    ('Logistic Regression', y_pred_lr),\n",
        "    ('Random Forest', y_pred_rf),\n",
        "    ('SVM', y_pred_svm),\n",
        "    ('Gradient Boosting', y_pred_gb),\n",
        "    ('KNN', y_pred_knn),\n",
        "    ('Naive Bayes', y_pred_nb)\n",
        "]\n",
        "\n",
        "for idx, (model_name, y_pred) in enumerate(predictions):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[idx],\n",
        "               xticklabels=['No Disease', 'Disease'],\n",
        "               yticklabels=['No Disease', 'Disease'])\n",
        "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold')\n",
        "    axes[idx].set_ylabel('Actual')\n",
        "    axes[idx].set_xlabel('Predicted')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed metrics for best model\n",
        "cm_best = confusion_matrix(y_test, y_pred_best)\n",
        "tn, fp, fn, tp = cm_best.ravel()\n",
        "\n",
        "print(f\"\\nDetailed Analysis - {best_model_name}:\")\n",
        "print(f\"  ‚Ä¢ True Positives (TP):  {tp} - Correctly identified disease cases\")\n",
        "print(f\"  ‚Ä¢ True Negatives (TN):  {tn} - Correctly identified healthy patients\")\n",
        "print(f\"  ‚Ä¢ False Positives (FP): {fp} - Healthy patients incorrectly flagged\")\n",
        "print(f\"  ‚Ä¢ False Negatives (FN): {fn} - Disease cases missed\")\n",
        "print(f\"\\n  ‚Ä¢ Sensitivity (Recall): {tp/(tp+fn):.4f} - {tp/(tp+fn)*100:.1f}% of disease cases detected\")\n",
        "print(f\"  ‚Ä¢ Specificity:          {tn/(tn+fp):.4f} - {tn/(tn+fp)*100:.1f}% of healthy correctly identified\")\n",
        "\n",
        "# ### 8.3 ROC Curve Comparison\n",
        "\n",
        "\"\"\"\n",
        "ROC curves show trade-off between true positive and false positive rates\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ROC CURVE ANALYSIS - ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot ROC curves\n",
        "roc_data = [\n",
        "    ('Logistic Regression', y_pred_proba_lr, lr_auc),\n",
        "    ('Random Forest', y_pred_proba_rf, rf_auc),\n",
        "    ('SVM', y_pred_proba_svm, svm_auc),\n",
        "    ('Gradient Boosting', y_pred_proba_gb, gb_auc),\n",
        "    ('KNN', y_pred_proba_knn, knn_auc),\n",
        "    ('Naive Bayes', y_pred_proba_nb, nb_auc)\n",
        "]\n",
        "\n",
        "for model_name, y_proba, auc_score in roc_data:\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {auc_score:.4f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5000)')\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
        "plt.title('ROC Curves - All Models', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üìä ROC-AUC Interpretation:\")\n",
        "print(f\"   ‚Ä¢ AUC = 1.0: Perfect classifier\")\n",
        "print(f\"   ‚Ä¢ AUC = 0.9-1.0: Excellent\")\n",
        "print(f\"   ‚Ä¢ AUC = 0.8-0.9: Good\")\n",
        "print(f\"   ‚Ä¢ AUC = 0.7-0.8: Fair\")\n",
        "print(f\"   ‚Ä¢ AUC = 0.5: Random guess\")\n",
        "\n",
        "# ### 8.4 Precision-Recall Curve\n",
        "\n",
        "\"\"\"\n",
        "Precision-Recall curves are useful for imbalanced datasets\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRECISION-RECALL CURVE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for model_name, y_proba, _ in roc_data:\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    plt.plot(recall, precision, linewidth=2, label=f'{model_name} (AP = {pr_auc:.4f})')\n",
        "\n",
        "plt.xlabel('Recall (Sensitivity)', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Precision-Recall Curves - All Models', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ### 8.5 Classification Report - Best Model\n",
        "\n",
        "\"\"\"\n",
        "Detailed classification metrics for the best model\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"CLASSIFICATION REPORT - {best_model_name.upper()}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(classification_report(y_test, y_pred_best,\n",
        "                          target_names=['No Disease', 'Heart Disease'],\n",
        "                          digits=4))\n",
        "\n",
        "# ---\n",
        "# ## 9. Cross-Validation\n",
        "# ### 9.1 K-Fold Cross-Validation for All Models\n",
        "\n",
        "\"\"\"\n",
        "Perform k-fold cross-validation to assess model stability\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"5-FOLD CROSS-VALIDATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "cv_results = {}\n",
        "\n",
        "# Models to cross-validate\n",
        "models_cv = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "for model_name, model in models_cv.items():\n",
        "    # Use scaled data for distance-based models\n",
        "    if model_name in ['Logistic Regression', 'SVM', 'KNN', 'Naive Bayes']:\n",
        "        X_cv = X_train_scaled_df\n",
        "    else:\n",
        "        X_cv = X_train\n",
        "\n",
        "    cv_scores = cross_val_score(model, X_cv, y_train, cv=cv, scoring='f1')\n",
        "    cv_results[model_name] = cv_scores\n",
        "\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(f\"  CV Scores: {cv_scores}\")\n",
        "    print(f\"  Mean F1:   {cv_scores.mean():.4f}\")\n",
        "    print(f\"  Std Dev:   {cv_scores.std():.4f}\")\n",
        "\n",
        "# Visualize CV results\n",
        "plt.figure(figsize=(14, 6))\n",
        "cv_df = pd.DataFrame(cv_results)\n",
        "\n",
        "bp = plt.boxplot([cv_df[col] for col in cv_df.columns],\n",
        "                labels=cv_df.columns,\n",
        "                patch_artist=True,\n",
        "                showmeans=True)\n",
        "\n",
        "# Color the boxes\n",
        "for patch, color in zip(bp['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "    patch.set_alpha(0.7)\n",
        "\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('F1-Score', fontsize=12)\n",
        "plt.title('5-Fold Cross-Validation Results', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---\n",
        "# ## 10. Key Insights and Clinical Findings\n",
        "# ### 10.1 Feature Importance - Clinical Insights\n",
        "\n",
        "\"\"\"\n",
        "Analyze which clinical factors are most predictive of heart disease\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)# Heart Failure Prediction Using Machine Learning\n",
        "# ================================================\n",
        "# Data Exploration, Model Comparison, and Insights\n",
        "#\n",
        "# Objective: Develop a machine learning pipeline to predict heart failure\n",
        "# using clinical data, compare multiple models, and derive actionable insights\n",
        "\n",
        "# ## 1. Project Setup and Environment Configuration\n",
        "# ### 1.1 Import Required Libraries\n",
        "\n",
        "\"\"\"\n",
        "This notebook implements a complete machine learning pipeline for heart failure prediction.\n",
        "We'll use:\n",
        "- Data manipulation: pandas, numpy\n",
        "- Visualization: matplotlib, seaborn\n",
        "- Machine Learning: scikit-learn\n",
        "- Statistical analysis: scipy\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
        "                                    StratifiedKFold, GridSearchCV, learning_curve)\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# ML Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, roc_curve, confusion_matrix,\n",
        "                            classification_report, precision_recall_curve, auc)\n",
        "\n",
        "# Feature Selection\n",
        "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"Set2\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"HEART FAILURE PREDICTION USING MACHINE LEARNING\")\n",
        "print(\"Data Exploration, Model Comparison, and Clinical Insights\")\n",
        "print(\"=\"*80)\n",
        "print(\"‚úì All libraries imported successfully\")\n",
        "print(f\"‚úì Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# ### 1.2 Configure Display Settings\n",
        "\n",
        "\"\"\"\n",
        "Set up display options for better readability and professional visualizations\n",
        "\"\"\"\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 11\n",
        "\n",
        "print(\"‚úì Display and visualization settings configured\")\n",
        "\n",
        "# ---\n",
        "# ## 2. Data Loading and Initial Exploration\n",
        "# ### 2.1 Load the Heart Failure Dataset\n",
        "\n",
        "\"\"\"\n",
        "Load the clinical heart failure dataset.\n",
        "This dataset contains various medical and demographic features\n",
        "used to predict the presence of heart disease.\n",
        "\"\"\"\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('heart_failure_data.csv')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET LOADED SUCCESSFULLY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total Patients: {len(df):,}\")\n",
        "print(f\"Total Features: {len(df.columns)}\")\n",
        "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# ### 2.2 Display Sample Data\n",
        "\n",
        "\"\"\"\n",
        "Examine the first and last few rows to understand data structure\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FIRST 10 PATIENT RECORDS\")\n",
        "print(\"=\"*80)\n",
        "print(df.head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LAST 5 PATIENT RECORDS\")\n",
        "print(\"=\"*80)\n",
        "print(df.tail())\n",
        "\n",
        "# ### 2.3 Dataset Structure and Information\n",
        "\n",
        "\"\"\"\n",
        "Comprehensive overview of dataset structure, data types, and completeness\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET STRUCTURE AND INFORMATION\")\n",
        "print(\"=\"*80)\n",
        "df.info()\n",
        "\n",
        "# ### 2.4 Feature Descriptions\n",
        "\n",
        "\"\"\"\n",
        "Understanding each clinical feature in the dataset:\n",
        "\"\"\"\n",
        "\n",
        "feature_descriptions = {\n",
        "    'Age': 'Age of the patient (years)',\n",
        "    'Sex': 'Sex of the patient (M: Male, F: Female)',\n",
        "    'ChestPainType': 'Type of chest pain (TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic)',\n",
        "    'RestingBP': 'Resting blood pressure (mm Hg)',\n",
        "    'Cholesterol': 'Serum cholesterol (mm/dl)',\n",
        "    'FastingBS': 'Fasting blood sugar > 120 mg/dl (1: Yes, 0: No)',\n",
        "    'RestingECG': 'Resting electrocardiogram results (Normal, ST, LVH)',\n",
        "    'MaxHR': 'Maximum heart rate achieved (60-202)',\n",
        "    'ExerciseAngina': 'Exercise-induced angina (Y: Yes, N: No)',\n",
        "    'Oldpeak': 'ST depression induced by exercise relative to rest',\n",
        "    'ST_Slope': 'Slope of peak exercise ST segment (Up, Flat, Down)',\n",
        "    'HeartDisease': 'Target variable - Presence of heart disease (1: Yes, 0: No)'\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLINICAL FEATURE DESCRIPTIONS\")\n",
        "print(\"=\"*80)\n",
        "for feature, description in feature_descriptions.items():\n",
        "    # Check for similar column names (case-insensitive, space handling)\n",
        "    matching_cols = [col for col in df.columns if feature.lower().replace('_', '').replace(' ', '')\n",
        "                    == col.lower().replace('_', '').replace(' ', '')]\n",
        "    if matching_cols:\n",
        "        print(f\"  ‚Ä¢ {matching_cols[0]:20s}: {description}\")\n",
        "\n",
        "# ### 2.5 Statistical Summary\n",
        "\n",
        "\"\"\"\n",
        "Statistical overview of numerical features:\n",
        "- Central tendency (mean, median)\n",
        "- Spread (std, min, max)\n",
        "- Distribution characteristics\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STATISTICAL SUMMARY OF NUMERICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "print(df.describe())\n",
        "\n",
        "# ### 2.6 Target Variable Analysis\n",
        "\n",
        "\"\"\"\n",
        "Analyze the distribution of heart disease (target variable).\n",
        "Understanding class balance is critical for model development.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TARGET VARIABLE ANALYSIS: HEART DISEASE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find the target column (might be named differently)\n",
        "target_col = None\n",
        "for col in df.columns:\n",
        "    if 'heart' in col.lower() and 'disease' in col.lower():\n",
        "        target_col = col\n",
        "        break\n",
        "    elif col.lower() in ['target', 'output', 'class']:\n",
        "        target_col = col\n",
        "        break\n",
        "\n",
        "if target_col:\n",
        "    target_counts = df[target_col].value_counts().sort_index()\n",
        "    target_pct = (target_counts / len(df) * 100).round(2)\n",
        "\n",
        "    print(f\"Target Variable: '{target_col}'\")\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    print(f\"  ‚Ä¢ No Heart Disease (0): {target_counts.get(0, 0):,} patients ({target_pct.get(0, 0)}%)\")\n",
        "    print(f\"  ‚Ä¢ Heart Disease (1):    {target_counts.get(1, 0):,} patients ({target_pct.get(1, 0)}%)\")\n",
        "\n",
        "    # Check for class imbalance\n",
        "    if len(target_counts) == 2:\n",
        "        imbalance_ratio = max(target_counts) / min(target_counts)\n",
        "        print(f\"\\n  Class Balance Ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "        if imbalance_ratio > 1.5:\n",
        "            print(f\"  ‚ö† Moderate class imbalance detected\")\n",
        "        elif imbalance_ratio > 2:\n",
        "            print(f\"  ‚ö† Significant class imbalance detected\")\n",
        "        else:\n",
        "            print(f\"  ‚úì Classes are well balanced\")\n",
        "\n",
        "    # Visualize target distribution\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Bar chart\n",
        "    axes[0].bar(['No Disease', 'Heart Disease'], target_counts.values,\n",
        "               color=['#2ecc71', '#e74c3c'], edgecolor='black', alpha=0.8)\n",
        "    axes[0].set_title('Heart Disease Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_ylabel('Number of Patients')\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(target_counts.values):\n",
        "        axes[0].text(i, v + 10, f'{v:,}\\n({target_pct.values[i]}%)',\n",
        "                    ha='center', fontweight='bold')\n",
        "\n",
        "    # Pie chart\n",
        "    axes[1].pie(target_counts.values, labels=['No Disease', 'Heart Disease'],\n",
        "               autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'],\n",
        "               startangle=90, explode=[0, 0.1])\n",
        "    axes[1].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö† Warning: Target variable not found. Please specify the correct column name.\")\n",
        "\n",
        "# ---\n",
        "# ## 3. Data Preprocessing\n",
        "# ### 3.1 Missing Values Analysis\n",
        "\n",
        "\"\"\"\n",
        "Identify and quantify missing data across all features.\n",
        "Missing data can significantly impact model performance.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MISSING VALUES ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "missing_data = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': df.isnull().sum(),\n",
        "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2),\n",
        "    'Data_Type': df.dtypes\n",
        "})\n",
        "\n",
        "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values(\n",
        "    'Missing_Percentage', ascending=False)\n",
        "\n",
        "if len(missing_data) > 0:\n",
        "    print(\"Columns with Missing Values:\")\n",
        "    print(missing_data.to_string(index=False))\n",
        "\n",
        "    # Visualize missing data\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.barh(missing_data['Column'], missing_data['Missing_Percentage'],\n",
        "            color='coral', edgecolor='black', alpha=0.8)\n",
        "    plt.xlabel('Missing Percentage (%)')\n",
        "    plt.title('Missing Data by Feature', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚úì Excellent! No missing values found in the dataset\")\n",
        "\n",
        "# ### 3.2 Handle Missing Values\n",
        "\n",
        "\"\"\"\n",
        "Strategy for handling missing values:\n",
        "1. Drop columns with >50% missing data\n",
        "2. Impute numerical features with median\n",
        "3. Impute categorical features with mode\n",
        "4. Remove rows with missing target variable\n",
        "\"\"\"\n",
        "\n",
        "df_clean = df.copy()\n",
        "initial_rows = len(df_clean)\n",
        "initial_cols = len(df_clean.columns)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HANDLING MISSING VALUES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Remove columns with >50% missing\n",
        "if len(missing_data) > 0:\n",
        "    high_missing = missing_data[missing_data['Missing_Percentage'] > 50]['Column'].tolist()\n",
        "    if len(high_missing) > 0:\n",
        "        df_clean = df_clean.drop(columns=high_missing)\n",
        "        print(f\"‚úì Dropped {len(high_missing)} columns with >50% missing: {high_missing}\")\n",
        "\n",
        "# Remove rows with missing target\n",
        "if target_col and df_clean[target_col].isnull().sum() > 0:\n",
        "    df_clean = df_clean.dropna(subset=[target_col])\n",
        "    print(f\"‚úì Removed {initial_rows - len(df_clean)} rows with missing target variable\")\n",
        "\n",
        "# Impute numerical features with median\n",
        "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if target_col in numerical_cols:\n",
        "    numerical_cols.remove(target_col)\n",
        "\n",
        "for col in numerical_cols:\n",
        "    if df_clean[col].isnull().sum() > 0:\n",
        "        median_val = df_clean[col].median()\n",
        "        df_clean[col].fillna(median_val, inplace=True)\n",
        "        print(f\"‚úì Imputed '{col}' missing values with median: {median_val:.2f}\")\n",
        "\n",
        "# Impute categorical features with mode\n",
        "categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
        "for col in categorical_cols:\n",
        "    if df_clean[col].isnull().sum() > 0:\n",
        "        mode_val = df_clean[col].mode()[0]\n",
        "        df_clean[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"‚úì Imputed '{col}' missing values with mode: {mode_val}\")\n",
        "\n",
        "print(f\"\\n‚úì Final dataset: {len(df_clean):,} patients √ó {len(df_clean.columns)} features\")\n",
        "print(f\"‚úì Data retention: {len(df_clean)/initial_rows*100:.1f}% of original rows\")\n",
        "\n",
        "# ### 3.3 Check for Duplicates\n",
        "\n",
        "\"\"\"\n",
        "Identify and remove duplicate patient records\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DUPLICATE RECORDS CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "duplicates = df_clean.duplicated().sum()\n",
        "print(f\"Duplicate rows found: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "    print(f\"‚úì Removed {duplicates} duplicate records\")\n",
        "    print(f\"‚úì Dataset now has {len(df_clean):,} unique patient records\")\n",
        "else:\n",
        "    print(\"‚úì No duplicate records found\")\n",
        "\n",
        "# ### 3.4 Data Type Corrections\n",
        "\n",
        "\"\"\"\n",
        "Ensure all features have appropriate data types\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA TYPE VALIDATION AND CORRECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Binary features should be 0/1 integers\n",
        "binary_features = ['FastingBS', target_col] if target_col else ['FastingBS']\n",
        "for col in binary_features:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = df_clean[col].astype(int)\n",
        "        print(f\"‚úì Converted '{col}' to integer type\")\n",
        "\n",
        "# Ensure numerical features are numeric\n",
        "for col in numerical_cols:\n",
        "    if col in df_clean.columns and df_clean[col].dtype == 'object':\n",
        "        try:\n",
        "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "            print(f\"‚úì Converted '{col}' to numeric type\")\n",
        "        except:\n",
        "            print(f\"‚ö† Could not convert '{col}' to numeric\")\n",
        "\n",
        "print(f\"\\n‚úì Data type validation completed\")\n",
        "\n",
        "# ### 3.5 Encode Categorical Variables\n",
        "\n",
        "\"\"\"\n",
        "Convert categorical text features to numerical format for machine learning.\n",
        "We'll use Label Encoding for binary categories and create dummy variables for multi-class.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENCODING CATEGORICAL VARIABLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df_encoded = df_clean.copy()\n",
        "label_encoders = {}\n",
        "\n",
        "# Get categorical columns\n",
        "categorical_columns = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Found {len(categorical_columns)} categorical columns: {categorical_columns}\")\n",
        "\n",
        "for col in categorical_columns:\n",
        "    unique_values = df_encoded[col].nunique()\n",
        "    print(f\"\\n  ‚Ä¢ {col}: {unique_values} unique values\")\n",
        "    print(f\"    Values: {df_encoded[col].unique().tolist()}\")\n",
        "\n",
        "    # Use Label Encoding\n",
        "    le = LabelEncoder()\n",
        "    df_encoded[col + '_encoded'] = le.fit_transform(df_encoded[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "    # Create mapping\n",
        "    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    print(f\"    Mapping: {mapping}\")\n",
        "\n",
        "print(f\"\\n‚úì Categorical encoding completed\")\n",
        "print(f\"‚úì Total features: {len(df_encoded.columns)}\")\n",
        "\n",
        "# ### 3.6 Feature Engineering\n",
        "\n",
        "\"\"\"\n",
        "Create additional features that might improve model performance:\n",
        "1. Age groups\n",
        "2. Blood pressure categories\n",
        "3. Cholesterol categories\n",
        "4. Heart rate zones\n",
        "5. Risk scores\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. Age groups\n",
        "if 'Age' in df_encoded.columns:\n",
        "    df_encoded['Age_Group'] = pd.cut(df_encoded['Age'],\n",
        "                                     bins=[0, 40, 55, 70, 100],\n",
        "                                     labels=['Young', 'Middle', 'Senior', 'Elderly'])\n",
        "    df_encoded['Age_Group_encoded'] = LabelEncoder().fit_transform(df_encoded['Age_Group'].astype(str))\n",
        "    print(\"‚úì Created Age_Group: Young (<40), Middle (40-55), Senior (55-70), Elderly (70+)\")\n",
        "\n",
        "# 2. Blood Pressure categories\n",
        "if 'RestingBP' in df_encoded.columns:\n",
        "    def categorize_bp(bp):\n",
        "        if bp < 120:\n",
        "            return 'Normal'\n",
        "        elif bp < 130:\n",
        "            return 'Elevated'\n",
        "        elif bp < 140:\n",
        "            return 'High_Stage1'\n",
        "        else:\n",
        "            return 'High_Stage2'\n",
        "\n",
        "    df_encoded['BP_Category'] = df_encoded['RestingBP'].apply(categorize_bp)\n",
        "    df_encoded['BP_Category_encoded'] = LabelEncoder().fit_transform(df_encoded['BP_Category'])\n",
        "    print(\"‚úì Created BP_Category: Normal, Elevated, High_Stage1, High_Stage2\")\n",
        "\n",
        "# 3. Cholesterol categories\n",
        "if 'Cholesterol' in df_encoded.columns:\n",
        "    # Filter out zero values (often missing data coded as 0)\n",
        "    df_encoded['Cholesterol_Valid'] = df_encoded['Cholesterol'].replace(0, np.nan)\n",
        "\n",
        "    def categorize_cholesterol(chol):\n",
        "        if pd.isna(chol):\n",
        "            return 'Unknown'\n",
        "        elif chol < 200:\n",
        "            return 'Desirable'\n",
        "        elif chol < 240:\n",
        "            return 'Borderline'\n",
        "        else:\n",
        "            return 'High'\n",
        "\n",
        "    df_encoded['Chol_Category'] = df_encoded['Cholesterol_Valid'].apply(categorize_cholesterol)\n",
        "    df_encoded['Chol_Category_encoded'] = LabelEncoder().fit_transform(df_encoded['Chol_Category'])\n",
        "    print(\"‚úì Created Chol_Category: Desirable (<200), Borderline (200-240), High (>240)\")\n",
        "\n",
        "# 4. Heart Rate zones\n",
        "if 'MaxHR' in df_encoded.columns and 'Age' in df_encoded.columns:\n",
        "    df_encoded['Max_HR_Expected'] = 220 - df_encoded['Age']\n",
        "    df_encoded['HR_Achievement_Pct'] = (df_encoded['MaxHR'] / df_encoded['Max_HR_Expected']) * 100\n",
        "    print(\"‚úì Created HR_Achievement_Pct: Percentage of maximum heart rate achieved\")\n",
        "\n",
        "# 5. Risk score (simple composite)\n",
        "if all(col in df_encoded.columns for col in ['Age', 'RestingBP', 'Cholesterol', 'FastingBS']):\n",
        "    df_encoded['Simple_Risk_Score'] = (\n",
        "        (df_encoded['Age'] > 55).astype(int) +\n",
        "        (df_encoded['RestingBP'] > 130).astype(int) +\n",
        "        (df_encoded['Cholesterol'] > 200).astype(int) +\n",
        "        df_encoded['FastingBS']\n",
        "    )\n",
        "    print(\"‚úì Created Simple_Risk_Score: Composite cardiovascular risk indicator (0-4)\")\n",
        "\n",
        "print(f\"\\n‚úì Feature engineering completed\")\n",
        "print(f\"‚úì Total features now: {len(df_encoded.columns)}\")\n",
        "\n",
        "# ---\n",
        "# ## 4. Exploratory Data Analysis (EDA)\n",
        "# ### 4.1 Univariate Analysis - Numerical Features\n",
        "\n",
        "\"\"\"\n",
        "Analyze the distribution of numerical features\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPLORATORY DATA ANALYSIS: NUMERICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
        "available_num_features = [f for f in numerical_features if f in df_encoded.columns]\n",
        "\n",
        "if len(available_num_features) > 0:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_num_features[:6]):\n",
        "        axes[idx].hist(df_encoded[feature], bins=30, color='steelblue',\n",
        "                      edgecolor='black', alpha=0.7)\n",
        "        axes[idx].set_title(f'Distribution of {feature}', fontweight='bold')\n",
        "        axes[idx].set_xlabel(feature)\n",
        "        axes[idx].set_ylabel('Frequency')\n",
        "        axes[idx].axvline(df_encoded[feature].mean(), color='red', linestyle='--',\n",
        "                         linewidth=2, label=f'Mean: {df_encoded[feature].mean():.1f}')\n",
        "        axes[idx].axvline(df_encoded[feature].median(), color='green', linestyle='--',\n",
        "                         linewidth=2, label=f'Median: {df_encoded[feature].median():.1f}')\n",
        "        axes[idx].legend(fontsize=8)\n",
        "        axes[idx].grid(alpha=0.3)\n",
        "\n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(available_num_features), 6):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ### 4.2 Univariate Analysis - Categorical Features\n",
        "\n",
        "\"\"\"\n",
        "Analyze the distribution of categorical features\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPLORATORY DATA ANALYSIS: CATEGORICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
        "# Find matching columns (handle different naming conventions)\n",
        "available_cat_features = []\n",
        "for feat in categorical_features:\n",
        "    matching = [col for col in df_clean.columns if feat.lower().replace('_', '') in col.lower().replace('_', '')]\n",
        "    if matching:\n",
        "        available_cat_features.append(matching[0])\n",
        "\n",
        "if len(available_cat_features) > 0:\n",
        "    n_features = len(available_cat_features)\n",
        "    n_cols = min(3, n_features)\n",
        "    n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
        "    if n_features == 1:\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_cat_features):\n",
        "        feature_counts = df_clean[feature].value_counts()\n",
        "        axes[idx].bar(range(len(feature_counts)), feature_counts.values,\n",
        "                     color='coral', edgecolor='black', alpha=0.8)\n",
        "        axes[idx].set_xticks(range(len(feature_counts)))\n",
        "        axes[idx].set_xticklabels(feature_counts.index, rotation=45, ha='right')\n",
        "        axes[idx].set_title(f'Distribution of {feature}', fontweight='bold')\n",
        "        axes[idx].set_ylabel('Count')\n",
        "        axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Add value labels\n",
        "        for i, v in enumerate(feature_counts.values):\n",
        "            axes[idx].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "    # Hide extra subplots\n",
        "    for idx in range(n_features, len(axes)):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ### 4.3 Bivariate Analysis - Features vs Heart Disease\n",
        "\n",
        "\"\"\"\n",
        "Analyze how each feature relates to heart disease outcome\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BIVARIATE ANALYSIS: FEATURES VS HEART DISEASE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if target_col:\n",
        "    # Numerical features vs target\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_num_features[:6]):\n",
        "        df_encoded.boxplot(column=feature, by=target_col, ax=axes[idx])\n",
        "        axes[idx].set_title(f'{feature} by Heart Disease Status', fontweight='bold')\n",
        "        axes[idx].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
        "        axes[idx].set_ylabel(feature)\n",
        "        plt.suptitle('')\n",
        "        axes[idx].grid(alpha=0.3)\n",
        "\n",
        "    for idx in range(len(available_num_features), 6):\n",
        "        axes[idx].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Statistical comparison\n",
        "    print(\"\\nMean Values by Heart Disease Status:\")\n",
        "    for feature in available_num_features:\n",
        "        no_disease_mean = df_encoded[df_encoded[target_col] == 0][feature].mean()\n",
        "        disease_mean = df_encoded[df_encoded[target_col] == 1][feature].mean()\n",
        "        difference = disease_mean - no_disease_mean\n",
        "        print(f\"  ‚Ä¢ {feature:15s}: No Disease={no_disease_mean:6.1f}, Disease={disease_mean:6.1f}, Diff={difference:+6.1f}\")\n",
        "\n",
        "# ### 4.4 Categorical Features vs Heart Disease\n",
        "\n",
        "\"\"\"\n",
        "Analyze relationship between categorical features and heart disease\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CATEGORICAL FEATURES VS HEART DISEASE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if target_col and len(available_cat_features) > 0:\n",
        "    n_features = min(4, len(available_cat_features))\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, feature in enumerate(available_cat_features[:4]):\n",
        "        crosstab = pd.crosstab(df_clean[feature], df_clean[target_col])\n",
        "        crosstab_pct = pd.crosstab(df_clean[feature], df_clean[target_col], normalize='index') * 100\n",
        "\n",
        "        crosstab.plot(kind='bar', ax=axes[idx], color=['#2ecc71', '#e74c3c'],\n",
        "                     edgecolor='black', alpha=0.8)\n",
        "        axes[idx].set_title(f'{feature} vs Heart Disease', fontweight='bold')\n",
        "        axes[idx].set_xlabel(feature)\n",
        "        axes[idx].set_ylabel('Count')\n",
        "        axes[idx].legend(['No Disease', 'Disease'])\n",
        "        axes[idx].tick_params(axis='x', rotation=45)\n",
        "        axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ### 4.5 Correlation Analysis\n",
        "\n",
        "\"\"\"\n",
        "Analyze correlations between all numerical features\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CORRELATION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select numerical columns including encoded categoricals\n",
        "numerical_for_corr = df_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Remove non-predictive columns\n",
        "exclude_cols = [col for col in numerical_for_corr if 'Group' in col and '_encoded' not in col]\n",
        "numerical_for_corr = [col for col in numerical_for_corr if col not in exclude_cols]\n",
        "\n",
        "if len(numerical_for_corr) > 2:\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = df_encoded[numerical_for_corr].corr()\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(16, 14))\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn', center=0,\n",
        "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
        "                annot_kws={'size': 8})\n",
        "    plt.title('Correlation Matrix - All Numerical Features', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Features most correlated with target\n",
        "    if target_col in corr_matrix.columns:\n",
        "        target_corr = corr_matrix[target_col].drop(target_col).sort_values(ascending=False)\n",
        "\n",
        "        print(\"\\nTop 15 Features Correlated with Heart Disease:\")\n",
        "        print(\"\\nPositive Correlations (increase disease risk):\")\n",
        "        for feature, corr in target_corr[target_corr > 0].head(10).items():\n",
        "            print(f\"   ‚Ä¢ {feature:30s}: {corr:.4f}\")\n",
        "\n",
        "        print(\"\\nNegative Correlations (decrease disease risk):\")\n",
        "        for feature, corr in target_corr[target_corr < 0].head(10).items():\n",
        "            print(f\"   ‚Ä¢ {feature:30s}: {corr:.4f}\")\n",
        "\n",
        "        # Visualize top correlations\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        top_15_corr = pd.concat([target_corr.head(8), target_corr.tail(7)])\n",
        "        colors = ['green' if x > 0 else 'red' for x in top_15_corr.values]\n",
        "        plt.barh(range(len(top_15_corr)), top_15_corr.values, color=colors,\n",
        "                edgecolor='black', alpha=0.8)\n",
        "        plt.yticks(range(len(top_15_corr)), top_15_corr.index)\n",
        "        plt.xlabel('Correlation Coefficient')\n",
        "        plt.title('Top 15 Features Correlated with Heart Disease', fontsize=14, fontweight='bold')\n",
        "        plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# ---\n",
        "# ## 5. Feature Selection and Preparation\n",
        "# ### 5.1 Prepare Features for Modeling\n",
        "\n",
        "\"\"\"\n",
        "Select final features for machine learning models:\n",
        "- Use encoded versions of categorical features\n",
        "- Remove original categorical columns\n",
        "- Remove intermediate feature engineering columns\n",
        "- Separate features (X) and target (y)\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n\" + \""
      ]
    }
  ]
}